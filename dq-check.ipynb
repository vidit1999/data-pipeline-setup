{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c47ba96-724a-40c0-9457-e1eca39ac423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from dateutil.parser import parse\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55690dc0-63fb-4ffa-b247-9a8b6d6c80fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'spark' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "    .appName(\"SparkLocalStackS3Integration\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ['AWS_ENDPOINT_URL'])\\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"])\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\",os.environ[\"AWS_SECRET_ACCESS_KEY\"])\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84a93b7-55b1-4424-995f-2a25d9f1da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(DISTINCT named_struct(dag_id, dag_id, task_id, task_id, run_id, run_id, map_index, map_index))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(DISTINCT named_struct(dag_id, dag_id, task_id, task_id, run_id, run_id, map_index, map_index))\n",
       "0                                                 55                                                   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"describe history delta.`s3a://my-storage-bucket/append-locations/postgres.public.task_instance_1_1` limit 10\").toPandas()\n",
    "spark.sql(f\"select count(distinct(dag_id, task_id , run_id , map_index)) from delta.`s3a://my-storage-bucket/append-locations/postgres.public.task_instance_1_1` limit 1\").toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cdf310-2d53-4b51-a887-0dc76b0a21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0        55"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delta.`s3a://my-storage-bucket/upsert-locations/postgres.public.task_instance_1_1` limit 10\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac0366e-ea98-4a45-a3c8-63bbc03c89d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to RDS database: airflow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, max as spark_max, abs as spark_abs, lit\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "RDS_DB_NAME = \"airflow\"\n",
    "RDS_USER = \"airflow\"\n",
    "RDS_PASSWORD = \"airflow\"\n",
    "RDS_HOST = \"postgres\"\n",
    "RDS_PORT = 5432\n",
    "\n",
    "def get_rds_connection():\n",
    "    \"\"\"Establishes a connection to the RDS database.\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=RDS_DB_NAME,\n",
    "            user=RDS_USER,\n",
    "            password=RDS_PASSWORD,\n",
    "            host=RDS_HOST,\n",
    "            port=RDS_PORT\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        print(f\"Successfully connected to RDS database: {RDS_DB_NAME}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to RDS: {e}\")\n",
    "        return None\n",
    "\n",
    "conn = get_rds_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0d2ab8-5213-456b-8b0b-a7c7b6495b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://my-storage-bucket/upsert-locations/postgres.public.task_instance_1_1\n"
     ]
    }
   ],
   "source": [
    "# TODO: fetch from config API\n",
    "pipeline_config_id = 1\n",
    "DATALAKE_PATH = requests.get(f\"http://backend:8000/configs/{pipeline_config_id}/\").json()[\"upsert_write_path\"]\n",
    "print(DATALAKE_PATH)\n",
    "datalake_df = spark.read.format('delta').load(DATALAKE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204aed79-8b53-42be-9a9b-74b15989b298",
   "metadata": {},
   "source": [
    "## Sampling record-to-record matching\n",
    "* For the provided columns it will perform,\n",
    "    * Schema matching\n",
    "    * Value matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851b18a1-a60f-487a-9ab6-c92d43a82bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_record_matching(datalake_df, fraction, rds_table, columns_to_check, pk_cols):\n",
    "    datalake_df = spark.read.format('delta').load(DATALAKE_PATH)\n",
    "    datalake_sample_df = datalake_df.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "\n",
    "    if datalake_sample_df.count() == 0:\n",
    "        print(f\"Zero sample records selected\")\n",
    "\n",
    "    columns_to_check = list(set(columns_to_check + pk_cols))\n",
    "\n",
    "    pk_filter = \" and \".join([f\"{pk_name} = %({pk_name})s\" for pk_name in pk_cols])\n",
    "    query = f\"SELECT * FROM {rds_table} where {pk_filter}\"\n",
    "\n",
    "    for row in datalake_sample_df.select(*columns_to_check).collect():\n",
    "        row_dict = row.asDict()\n",
    "        print(\"Datalake row\", row_dict)\n",
    "        pk_vals = {k: v for k,v in row_dict.items() if k in pk_cols}\n",
    "\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query, row_dict)\n",
    "            row = cur.fetchone()\n",
    "\n",
    "            if not row:\n",
    "                print(f\"Warning: No records found in rds table for {pk_vals}\")\n",
    "                continue\n",
    "            \n",
    "            column_names = [desc[0] for desc in cur.description]\n",
    "            row_dict_rds = dict(zip(column_names, row))\n",
    "\n",
    "        for c in columns_to_check:\n",
    "            if row_dict_rds.get(c) != row_dict.get(c):\n",
    "                print(f\"Error: Found mismatch for column {c}. RDS: {row_dict_rds.get(c)}. Datalake: {row_dict.get(c)}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Matched for {pk_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11736d53-455e-4154-9093-c8a727505ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datalake row {'custom_operator_name': None, 'run_id': 'manual__2025-06-15T06:50:31.485049+00:00', 'try_number': 1, 'dag_id': 'frequency_one', 'map_index': -1, 'task_id': 'docker_task_append_1_postgres.public.task_instance'}\n",
      "Matched for {'run_id': 'manual__2025-06-15T06:50:31.485049+00:00', 'dag_id': 'frequency_one', 'map_index': -1, 'task_id': 'docker_task_append_1_postgres.public.task_instance'}\n",
      "Datalake row {'custom_operator_name': None, 'run_id': 'scheduled__2025-06-15T06:49:37.820221+00:00', 'try_number': 1, 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Matched for {'run_id': 'scheduled__2025-06-15T06:49:37.820221+00:00', 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Datalake row {'custom_operator_name': None, 'run_id': 'scheduled__2025-06-15T06:49:46.194138+00:00', 'try_number': 1, 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Matched for {'run_id': 'scheduled__2025-06-15T06:49:46.194138+00:00', 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Datalake row {'custom_operator_name': None, 'run_id': 'scheduled__2025-06-15T06:49:43.993337+00:00', 'try_number': 1, 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Matched for {'run_id': 'scheduled__2025-06-15T06:49:43.993337+00:00', 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sampling_record_matching(datalake_df, 0.1, \"task_instance\", [\"custom_operator_name\", \"run_id\", \"try_number\"], [\"dag_id\",\"task_id\",\"run_id\",\"map_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989894cb-a207-4127-8aaf-c0c7e4731ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datalake row {'custom_operator_name': None, 'run_id': 'manual__2025-06-15T06:50:31.485049+00:00', 'end_date': None, 'try_number': 1, 'dag_id': 'frequency_one', 'map_index': -1, 'task_id': 'docker_task_append_1_postgres.public.task_instance'}\n",
      "Error: Found mismatch for column end_date. RDS: 2025-06-15 06:50:53.221776+00:00. Datalake: None\n",
      "Datalake row {'custom_operator_name': None, 'run_id': 'scheduled__2025-06-15T06:49:37.820221+00:00', 'end_date': '2025-06-15T06:49:41.453703Z', 'try_number': 1, 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Error: Found mismatch for column end_date. RDS: 2025-06-15 06:49:41.453703+00:00. Datalake: 2025-06-15T06:49:41.453703Z\n",
      "Datalake row {'custom_operator_name': None, 'run_id': 'scheduled__2025-06-15T06:49:46.194138+00:00', 'end_date': '2025-06-15T06:49:48.275005Z', 'try_number': 1, 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Error: Found mismatch for column end_date. RDS: 2025-06-15 06:49:48.275005+00:00. Datalake: 2025-06-15T06:49:48.275005Z\n",
      "Datalake row {'custom_operator_name': None, 'run_id': 'scheduled__2025-06-15T06:49:43.993337+00:00', 'end_date': '2025-06-15T06:49:46.758774Z', 'try_number': 1, 'dag_id': 'continuous_python_dag', 'map_index': -1, 'task_id': 'run_my_continuous_task'}\n",
      "Error: Found mismatch for column end_date. RDS: 2025-06-15 06:49:46.758774+00:00. Datalake: 2025-06-15T06:49:46.758774Z\n"
     ]
    }
   ],
   "source": [
    "sampling_record_matching(datalake_df, 0.1, \"task_instance\", [\"custom_operator_name\", \"run_id\", \"try_number\", \"end_date\"], [\"dag_id\",\"task_id\",\"run_id\",\"map_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd338c-02d6-4ad4-b460-f96d8f58d1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d7805c0-61cb-466e-b5e5-ea9ec8cf3b47",
   "metadata": {},
   "source": [
    "## Heuristics count matching check\n",
    "* This will try to check if a given row is there datalake table or not.\n",
    "* It also considers any expected lag due to schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aa3365b-a3e0-4b96-a014-73c7d46ca11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristics_count_matching(datalake_df, rds_table, pk_cols, updated_at_field, frequency):\n",
    "    datalake_df = spark.read.format('delta').load(DATALAKE_PATH)\n",
    "\n",
    "    pk_cols_select = \", \".join(pk_cols)\n",
    "\n",
    "    #### NOTE\n",
    "    # This is not the most optimized query\n",
    "    query = f\"\"\"\n",
    "        SELECT {pk_cols_select} FROM {rds_table} where {updated_at_field} IS NOT NULL\n",
    "        AND {updated_at_field} < now() - INTERVAL '{frequency} hour'\n",
    "        ORDER BY random() LIMIT 5\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "        row_dict_rds = [dict(zip(column_names, row)) for row in rows]\n",
    "\n",
    "    for row_rds in row_dict_rds:\n",
    "        tmp_datalake_df = datalake_df\n",
    "        for pk_name, pk_val in row_rds.items():\n",
    "            tmp_datalake_df = tmp_datalake_df.filter(F.col(pk_name) == pk_val)\n",
    "        count = tmp_datalake_df.count()\n",
    "        if count == 1:\n",
    "            print(f\"Single matched redord found for {row_rds}\")\n",
    "        elif count > 1:\n",
    "            print(f\"Warning: Multiple matched redords found for {row_rds}\")\n",
    "        else:\n",
    "            print(f\"Error: No matched record found for {row_rds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5b0d7a9-4d36-4379-b227-9db9e1f9db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics_count_matching(datalake_df, \"task_instance\", [\"dag_id\",\"task_id\",\"run_id\",\"map_index\"], \"end_date\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3d20d-af1a-4f81-be80-00edc33171b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18217491-9680-4570-9258-8d5ada041585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eace21cb-efe1-4af7-a7e7-17deb390839e",
   "metadata": {},
   "source": [
    "## Table Lag checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04272c23-b288-46db-a55d-f10c628631e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_lag_checing(datalake_df, rds_table, pk_cols, updated_at_field, frequency):\n",
    "    datalake_df = spark.read.format('delta').load(DATALAKE_PATH)\n",
    "\n",
    "    pk_cols_select = \", \".join(pk_cols)\n",
    "\n",
    "    #### NOTE\n",
    "    # This is not the most optimized query\n",
    "    query = f\"\"\"\n",
    "        SELECT {pk_cols_select}, {updated_at_field} FROM {rds_table}\n",
    "        where {updated_at_field} IS NOT NULL\n",
    "        AND {updated_at_field} < now() - INTERVAL '{frequency} hour'\n",
    "        ORDER BY {updated_at_field} DESC LIMIT 1\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query)\n",
    "        row = cur.fetchone()\n",
    "        if not row:\n",
    "            print(f\"Warning: No records found in RDS table\")\n",
    "            return\n",
    "        \n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "        row_dict_rds = dict(zip(column_names, row))\n",
    "\n",
    "    for pk_name, pk_val in row_dict_rds.items():\n",
    "        datalake_df = datalake_df.filter(F.col(pk_name) == pk_val)\n",
    "\n",
    "    dl_rows = datalake_df.orderBy(F.col(updated_at_field).desc()).collect()\n",
    "    \n",
    "    if len(dl_rows) == 0:\n",
    "        print(f\"Error: No records found for {row_dict_rds}\")\n",
    "        return\n",
    "    elif len(dl_rows) > 1:\n",
    "        print(f\"Error: Multiple records found for {row_dict_rds} taking the latest one.\")\n",
    "    \n",
    "    dl_row = dl_rows[0]\n",
    "\n",
    "    dl_row_updated_at = parse(dl_row[updated_at_field]) if isinsance(dl_row[updated_at_field], str) else dl_row[updated_at_field]\n",
    "\n",
    "    if (row_dict_rds[updated_at_field] > dl_row_updated_at):\n",
    "        print(f\"Error: Found Lag for {row_dict_rds}. RDS updated at {row_dict_rds[updated_at_field]}. Datalake update at {dl_row_updated_at}\")\n",
    "    else:\n",
    "        print(f\"No Lag found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "612d26d3-4386-4894-885b-d5c9aa808ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No records found for {'dag_id': 'continuous_python_dag', 'task_id': 'run_my_continuous_task', 'run_id': 'scheduled__2025-06-15T07:05:23.547339+00:00', 'map_index': -1, 'end_date': datetime.datetime(2025, 6, 15, 7, 5, 26, 279552, tzinfo=datetime.timezone.utc)}\n"
     ]
    }
   ],
   "source": [
    "table_lag_checing(datalake_df, \"task_instance\", [\"dag_id\",\"task_id\",\"run_id\",\"map_index\"], \"end_date\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a785777b-dbf3-4980-91cc-08eec65c06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|max(end_date)              |\n",
      "+---------------------------+\n",
      "|2025-06-15T06:50:41.839462Z|\n",
      "+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select max(end_date) from delta.`s3a://my-storage-bucket/upsert-locations/postgres.public.task_instance_1_1`\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab776ea-e449-4ecd-b73a-515f74dba7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
